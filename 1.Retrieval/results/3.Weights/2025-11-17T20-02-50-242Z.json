{
  "selectedSource": {
    "arxivId": "2504.21776",
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability"
  },
  "target": {
    "arxivId": "2502.17419",
    "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
  },
  "scores": {
    "rank": 8,
    "ordered": [
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "positiveScores": {
          "arxivId": "2210.03629",
          "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 8
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 9
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 8
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 10
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 7
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2210.03629",
          "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 7
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 9
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 7
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 239.5
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "positiveScores": {
          "arxivId": "2508.06471",
          "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 9
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 9
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 7
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 6
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 7
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2508.06471",
          "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 6
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 5
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 6
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 234
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "positiveScores": {
          "arxivId": "2507.05241",
          "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 9
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 8
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 6
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 5
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 7
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2507.05241",
          "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 6
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 5
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 6
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 4
            }
          ]
        },
        "score": 204.5
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "positiveScores": {
          "arxivId": "2507.02592",
          "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 8
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 9
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 7
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 6
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 7
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2507.02592",
          "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 7
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 9
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 192
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "positiveScores": {
          "arxivId": "2409.12941",
          "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 5
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 7
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 9
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 5
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 6
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2409.12941",
          "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 4
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 3
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 4
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 9
            }
          ]
        },
        "score": 191.5
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "positiveScores": {
          "arxivId": "2311.12983",
          "title": "GAIA: a benchmark for General AI Assistants",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 7
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 7
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 10
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 4
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 6
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2311.12983",
          "title": "GAIA: a benchmark for General AI Assistants",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 6
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 3
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 5
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 9
            }
          ]
        },
        "score": 190
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "positiveScores": {
          "arxivId": "2501.12948",
          "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 5
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 9
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 6
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 9
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 6
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2501.12948",
          "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 7
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 9
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 7
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 183
      },
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
        "positiveScores": {
          "arxivId": "2504.21776",
          "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 8
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 9
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 5
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 5
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 8
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2504.21776",
          "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 7
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 8
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 180.5
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "positiveScores": {
          "arxivId": "2502.01600",
          "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 9
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 7
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 5
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 8
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 6
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2502.01600",
          "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 9
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 7
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 172.5
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "positiveScores": {
          "arxivId": "2501.14249",
          "title": "Humanity's Last Exam",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 5
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 7
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 8
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 4
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 6
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2501.14249",
          "title": "Humanity's Last Exam",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 5
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 4
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 5
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 6
            }
          ]
        },
        "score": 169
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "positiveScores": {
          "arxivId": "2501.07572",
          "title": "WebWalker: Benchmarking LLMs in Web Traversal",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 7
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 6
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 9
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 3
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 5
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2501.07572",
          "title": "WebWalker: Benchmarking LLMs in Web Traversal",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 5
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 2
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 5
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 9
            }
          ]
        },
        "score": 166
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
        "positiveScores": {
          "arxivId": "2502.09977",
          "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 5
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 6
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 9
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 4
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 5
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2502.09977",
          "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 4
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 3
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 4
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 9
            }
          ]
        },
        "score": 164
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "positiveScores": {
          "arxivId": "2504.12516",
          "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 7
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 6
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 9
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 3
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 5
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2504.12516",
          "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 5
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 2
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 5
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 10
            }
          ]
        },
        "score": 163
      },
      {
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling",
        "positiveScores": {
          "arxivId": "2509.13311",
          "title": "Towards General Agentic Intelligence via Environment Scaling",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 9
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 6
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 4
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 4
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 8
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2509.13311",
          "title": "Towards General Agentic Intelligence via Environment Scaling",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 6
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 7
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 7
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 158
      },
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
        "positiveScores": {
          "arxivId": "2509.13309",
          "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 9
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 8
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 5
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 4
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 7
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2509.13309",
          "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 7
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 9
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 157
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "positiveScores": {
          "arxivId": "2402.03300",
          "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 3
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 9
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 8
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 5
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 5
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2402.03300",
          "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 9
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 7
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 5
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 4
            }
          ]
        },
        "score": 140.5
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
        "positiveScores": {
          "arxivId": "2509.13312",
          "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 7
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 7
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 4
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 3
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 9
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2509.13312",
          "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 7
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 7
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 139.5
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
        "positiveScores": {
          "arxivId": "2509.13313",
          "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 8
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 7
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 3
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 5
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 7
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2509.13313",
          "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 7
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 8
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 137
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
        "positiveScores": {
          "arxivId": "2507.15061",
          "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 8
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 6
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 3
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 5
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 7
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2507.15061",
          "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 7
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 8
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 127
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "positiveScores": {
          "arxivId": "2509.02544",
          "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 7
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 6
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 5
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 7
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 5
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2509.02544",
          "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 9
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 9
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 121
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
        "positiveScores": {
          "arxivId": "2509.13305",
          "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 7
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 6
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 3
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 8
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 5
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2509.13305",
          "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 9
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 9
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 115
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI",
        "positiveScores": {
          "arxivId": "2506.02153",
          "title": "Small Language Models are the Future of Agentic AI",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 8
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 5
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 3
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 4
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 5
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2506.02153",
          "title": "Small Language Models are the Future of Agentic AI",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 7
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 5
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 6
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 115
      },
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
        "positiveScores": {
          "arxivId": "2504.19314",
          "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 5
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 5
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 9
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 2
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 3
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2504.19314",
          "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 5
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 2
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 5
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 9
            }
          ]
        },
        "score": 114
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "positiveScores": {
          "arxivId": "2505.22648",
          "title": "WebDancer: Towards Autonomous Information Seeking Agency",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 7
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 6
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 3
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 4
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 7
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2505.22648",
          "title": "WebDancer: Towards Autonomous Information Seeking Agency",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 8
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 7
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 8
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 110
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "positiveScores": {
          "arxivId": "2503.14476",
          "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 5
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 5
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 3
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 8
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 4
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2503.14476",
          "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 7
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 9
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 7
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 94.5
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training",
        "positiveScores": {
          "arxivId": "2509.13310",
          "title": "Scaling Agents via Continual Pre-training",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 8
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 5
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 2
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 5
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 5
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2509.13310",
          "title": "Scaling Agents via Continual Pre-training",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 7
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 9
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 7
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 92.5
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "positiveScores": {
          "arxivId": "2505.09388",
          "title": "Qwen3 Technical Report",
          "scores": [
            {
              "theme": "Agentic AI and Long-Horizon Tasks",
              "score": 4
            },
            {
              "theme": "Reasoning Capabilities in LLMs",
              "score": 5
            },
            {
              "theme": "Evaluation and Benchmarking of Reasoning",
              "score": 4
            },
            {
              "theme": "Methods for Improving Reasoning",
              "score": 3
            },
            {
              "theme": "Advanced Information Processing and Intelligence",
              "score": 4
            }
          ]
        },
        "negativeScores": {
          "arxivId": "2505.09388",
          "title": "Qwen3 Technical Report",
          "scores": [
            {
              "theme": "Specialization vs. Broad Survey",
              "score": 6
            },
            {
              "theme": "Methodology vs. Foundational Concepts",
              "score": 4
            },
            {
              "theme": "Specific Agent Architectures vs. General LLM Reasoning",
              "score": 5
            },
            {
              "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
              "score": 3
            }
          ]
        },
        "score": 88
      }
    ],
    "reflection": {
      "summaryResults": {
        "explanation_themes": [
          {
            "theme": "Agentic AI and Long-Horizon Tasks",
            "description": "Several papers focus on AI agents, their development, scaling, and performance in complex, long-horizon tasks. These tasks often inherently require sophisticated reasoning capabilities, making them relevant to a survey on reasoning in LLMs, especially concerning how LLMs can be used to create more capable agents."
          },
          {
            "theme": "Reasoning Capabilities in LLMs",
            "description": "Many papers directly mention or imply the enhancement, evaluation, or application of reasoning capabilities within LLMs. This includes specific types of reasoning like mathematical or scientific reasoning, as well as general reasoning for complex problem-solving and information processing."
          },
          {
            "theme": "Evaluation and Benchmarking of Reasoning",
            "description": "Several papers introduce benchmarks or evaluation methodologies designed to test and quantify the reasoning abilities of LLMs. Topics like GAIA benchmarks, ReAct framework, and evaluations of RAG systems or web browsing agents fall under this theme, as they are crucial for understanding and advancing LLM reasoning."
          },
          {
            "theme": "Methods for Improving Reasoning",
            "description": "Some papers discuss specific techniques or approaches aimed at improving reasoning in LLMs. This includes the use of reinforcement learning, incentivizing reasoning capability, and frameworks like ReAct that combine reasoning with action, all of which are directly pertinent to the development of reasoning LLMs."
          },
          {
            "theme": "Advanced Information Processing and Intelligence",
            "description": "Papers discussing 'structuring web-scale evidence', 'deep research', 'search intelligence', and 'General Agentic Intelligence' point towards advanced information processing and AI capabilities that rely heavily on reasoning. These aspects are likely to be covered in a comprehensive survey on reasoning LLMs."
          }
        ],
        "contrastive_themes": [
          {
            "theme": "Specialization vs. Broad Survey",
            "description": "A recurring contrast is that many of the related papers focus on highly specific applications, methodologies, or benchmarks (e.g., RAG, mathematical reasoning, web browsing agents, RL for agents) rather than providing a broad, general overview of reasoning across the entire LLM landscape. The target paper, being a survey, is expected to cover a wider spectrum."
          },
          {
            "theme": "Methodology vs. Foundational Concepts",
            "description": "Some papers contribute specific techniques or frameworks (e.g., continual pre-training for scaling agents, specific RL approaches, ReAct framework) for reasoning. However, a survey paper would likely delve deeper into the underlying theoretical concepts, different types of reasoning, and a historical progression of ideas, rather than just detailing specific implementation methodologies."
          },
          {
            "theme": "Specific Agent Architectures vs. General LLM Reasoning",
            "description": "Several papers focus on the development and capabilities of particular types of agents or specific models. While these agents utilize reasoning, the contrast lies in the fact that a general survey on reasoning LLMs would likely cover reasoning capabilities across various LLM architectures and tasks, not just those optimized for specific agentic roles or models."
          },
          {
            "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
            "description": "While benchmarks are important for measuring reasoning, papers focusing primarily on introducing or evaluating against specific benchmarks might not offer the same depth on the fundamental mechanisms and diverse forms of reasoning that a comprehensive survey would explore."
          }
        ]
      },
      "weights": {
        "positive_weights": [
          {
            "theme": "Agentic AI and Long-Horizon Tasks",
            "weight": 8,
            "explanation": "The target paper is a survey on reasoning LLMs, and agentic AI often relies heavily on advanced reasoning for long-horizon tasks. This theme directly supports the idea that LLMs are being used to build more capable agents, which is a key area of LLM development."
          },
          {
            "theme": "Reasoning Capabilities in LLMs",
            "weight": 10,
            "explanation": "This theme is directly aligned with the core subject of the target paper, which is a survey on reasoning LLMs. Any paper discussing or evaluating reasoning capabilities in LLMs is highly likely to be related."
          },
          {
            "theme": "Evaluation and Benchmarking of Reasoning",
            "weight": 7.5,
            "explanation": "Evaluating and benchmarking reasoning is crucial for understanding progress in LLMs. Papers that introduce or use such methods are directly relevant to a survey that aims to capture the state of the art in LLM reasoning."
          },
          {
            "theme": "Methods for Improving Reasoning",
            "weight": 9,
            "explanation": "Techniques and frameworks designed to improve LLM reasoning are fundamental to the advancement of the field. A survey on reasoning LLMs would naturally cover these methods and their impact."
          },
          {
            "theme": "Advanced Information Processing and Intelligence",
            "weight": 8.5,
            "explanation": "Concepts like 'structuring web-scale evidence' and 'search intelligence' imply complex reasoning processes. These advanced capabilities are a significant outcome of sophisticated LLM reasoning and are likely to be discussed in a comprehensive survey."
          }
        ],
        "negative_weights": [
          {
            "theme": "Specialization vs. Broad Survey",
            "weight": 6,
            "explanation": "While the target paper is a survey, many related papers might focus on specific applications. This theme highlights a potential difference in scope, suggesting that papers with extreme specialization might be less relevant than those with a broader focus on reasoning itself, but there's still overlap as these specialized areas demonstrate reasoning applications."
          },
          {
            "theme": "Methodology vs. Foundational Concepts",
            "weight": 5,
            "explanation": "A survey paper is likely to cover foundational concepts more broadly than papers that focus on implementing a specific methodology. This contrast suggests that papers heavily focused on detailed implementation might be less central to a broad survey, but the methodologies themselves are still part of the reasoning landscape."
          },
          {
            "theme": "Specific Agent Architectures vs. General LLM Reasoning",
            "weight": 4,
            "explanation": "The target paper is about general LLM reasoning, not necessarily specific agent architectures. Papers focusing solely on niche agent models might be less relevant than those discussing reasoning across various LLM contexts."
          },
          {
            "theme": "Evaluation Benchmarks vs. Core Reasoning Mechanisms",
            "weight": 3,
            "explanation": "Papers that are *primarily* about introducing a benchmark, rather than exploring the core reasoning mechanisms that the benchmark is designed to test, might have less direct relevance to a survey focused on the 'why' and 'how' of LLM reasoning itself."
          }
        ]
      }
    }
  },
  "ranking": {
    "rank": 2,
    "ordered": [
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "score": 0.25386566928864385
      },
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
        "score": 0.27599582594190375
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "score": 0.3679109035256234
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "score": 0.37303393792696127
      },
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
        "score": 0.3759657444125101
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI",
        "score": 0.4202829073080502
      },
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
        "score": 0.4347479922053078
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
        "score": 0.4643980933720496
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "score": 0.48300889601366415
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "score": 0.4939274716269314
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
        "score": 0.5173329189759404
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "score": 0.524751686604028
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "score": 0.5291560242537209
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "score": 0.5350553675296188
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "score": 0.5732112061722986
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "score": 0.5799707066513253
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
        "score": 0.5820469784015232
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
        "score": 0.5882716854261791
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
        "score": 0.6029388874339785
      },
      {
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling",
        "score": 0.6221174715174804
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "score": 0.6233016654133379
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "score": 0.6271484771657236
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "score": 0.6493208678303635
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "score": 0.650395757517802
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "score": 0.6509785754619282
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training",
        "score": 0.6619444899628374
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "score": 0.7726806671273224
      }
    ]
  },
  "semanticRanking": {
    "rank": 1,
    "ordered": [
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
        "score": 0.5085425924709291
      },
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "score": 0.5606257574401705
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "score": 0.6428363217244426
      },
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
        "score": 0.6483240984044198
      },
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
        "score": 0.7123160311867227
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "score": 0.7157294560839053
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "score": 0.7247861235425211
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI",
        "score": 0.7273644559702732
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
        "score": 0.7930993741772798
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
        "score": 0.8171353079820031
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "score": 0.8255288522632934
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "score": 0.8302111644567005
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
        "score": 0.835576937901148
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "score": 0.8378452628026714
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "score": 0.8578770902413344
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
        "score": 0.8903304055260559
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "score": 0.8929931688885828
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "score": 0.9175602222117988
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "score": 0.9208185794628619
      },
      {
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling",
        "score": 0.9499325063011688
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "score": 0.9552232157401482
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "score": 0.9577491128524472
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
        "score": 0.9609457158650756
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "score": 0.9957638371039806
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training",
        "score": 1.0181291501844005
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "score": 1.1502382405597316
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "score": 1.367972076861303
      }
    ]
  },
  "refs": {
    "seed": {
      "arxivId": "2510.24701",
      "title": "Tongyi DeepResearch Technical Report"
    },
    "sources": [
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents"
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning"
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research"
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training"
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization"
      },
      {
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling"
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning"
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models"
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization"
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?"
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent"
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI"
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency"
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report"
      },
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability"
      },
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese"
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents"
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing"
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents"
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam"
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal"
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants"
      },
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      }
    ],
    "selectedSource": {
      "arxivId": "2504.21776",
      "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability"
    },
    "target": {
      "arxivId": "2502.17419",
      "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
    }
  }
}