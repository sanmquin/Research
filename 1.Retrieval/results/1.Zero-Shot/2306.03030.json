{
  "references": {
    "seed": {
      "arxivId": "2510.24701",
      "title": "Tongyi DeepResearch Technical Report"
    },
    "sources": [
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents"
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning"
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research"
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training"
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization"
      },
      {
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling"
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning"
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models"
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization"
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?"
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent"
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI"
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency"
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report"
      },
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability"
      },
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese"
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents"
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing"
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents"
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam"
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal"
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants"
      },
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      }
    ],
    "selectedSource": {
      "arxivId": "2504.19314",
      "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese"
    },
    "target": {
      "arxivId": "2306.03030",
      "title": "Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset"
    }
  },
  "embeddings": {
    "rank": 1,
    "ordered": [
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
        "distance": 0.3059470758365269
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "distance": 0.45153893770036047
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
        "distance": 0.49260533390430694
      },
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
        "distance": 0.5264335539238405
      },
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "distance": 0.5275366338103507
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "distance": 0.5453139809534866
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
        "distance": 0.5491945557415876
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI",
        "distance": 0.5495982755043409
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "distance": 0.5523933346639718
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "distance": 0.5633272547326769
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
        "distance": 0.6098704673777369
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "distance": 0.6261214656572001
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "distance": 0.664633521776017
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "distance": 0.6684428439379069
      },
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
        "distance": 0.6751436104526232
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "distance": 0.6791704837038246
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "distance": 0.6890292123139365
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
        "distance": 0.6929752497466324
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "distance": 0.6972088418075129
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training",
        "distance": 0.6974449305286665
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "distance": 0.6998818665160942
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "distance": 0.7080571460589544
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "distance": 0.7259395855234089
      },
      {
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling",
        "distance": 0.7359079136547179
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "distance": 0.7494213505306623
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "distance": 0.7503495496346101
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
        "distance": 0.7791704013569718
      }
    ]
  },
  "llm": {
    "rank": 2,
    "ordered": [
      {
        "arxivId": "2306.03030",
        "title": "Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset",
        "score": 10,
        "reason": "This is the TARGET paper. The task is to rank papers that bridge from START to TARGET. Therefore, the TARGET paper itself should be ranked highest as it represents the destination."
      },
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
        "score": 9,
        "reason": "This paper directly addresses benchmarking LLMs on a Chinese dataset, which aligns well with the Chinese Medical Exam Dataset mentioned in the target paper. It also focuses on web browsing ability, a common component in agent tasks."
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "score": 8,
        "reason": "The title mentions 'Humanity's Last Exam' and 'General-Purpose Scientific AI Agents', which suggests a focus on rigorous evaluation and advanced reasoning, aligning with the spirit of benchmarking complex exams like CMExam."
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "score": 7,
        "reason": "This paper introduces a challenging benchmark, which is conceptually similar to the CMExam dataset used for evaluating LLMs. It indicates a focus on high-stakes, comprehensive evaluations."
      },
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
        "score": 7,
        "reason": "Focuses on 'unbounded reasoning capability' and 'Long-Horizon Agents'. This suggests advanced reasoning skills which are crucial for tackling complex exams, and the 'long-horizon' aspect might relate to the breadth of knowledge tested."
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
        "score": 7,
        "reason": "Emphasizes 'Deep Research' and 'Web-Scale Evidence'. This relates to gathering and synthesizing information, a skill likely required for a comprehensive medical exam. The 'dynamic outlines' might imply structured approaches to complex tasks."
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
        "score": 7,
        "reason": "Addresses 'Long-Horizon Search Intelligence' and 'Context Summarization'. These are key abilities for agents that need to process and synthesize large amounts of information, relevant for a comprehensive exam."
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "score": 6,
        "reason": "While focused on GUI agents, the 'Advancing' and 'Multi-Turn Reinforcement Learning' aspects suggest improvements in agent capabilities and learning, which can be applied to complex reasoning tasks like exams."
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "score": 6,
        "reason": "Highlights 'Reasoning' and 'Foundation Models', which are core components for developing capable AI agents that can perform well on complex benchmarks."
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
        "score": 6,
        "reason": "Focuses on 'Data Synthesizing' and 'Information-Seeking'. This is relevant for agents that need to process and generate information, a skill applicable to exam scenarios."
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "score": 6,
        "reason": "Mentions 'Super-human Reasoning' for web agents. Advanced reasoning is directly applicable to performing well on difficult benchmarks like CMExam."
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "score": 6,
        "reason": "Focuses on 'Autonomous Information Seeking Agency'. This is a general capability for advanced AI agents that might be tested in comprehensive exams."
      },
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
        "score": 6,
        "reason": "Combines 'Large Reasoning Models' with 'Deep Research Capability'. This is highly relevant to agents that need to perform complex reasoning and information retrieval, similar to an exam."
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "score": 5,
        "reason": "Introduces a 'Challenging Benchmark' for 'Browsing Agents'. While not specific to medical exams, it suggests a focus on evaluating agent performance on difficult tasks."
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "score": 5,
        "reason": "Focuses on 'Reinforcement Learning' for LLMs. RL is a common method for training agents to perform complex tasks and improve their performance, which could be applied to exam-taking."
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
        "score": 5,
        "reason": "This paper benchmarks important LLM capabilities like RAG and long-context, which are relevant for agents that need to access and utilize extensive knowledge bases, like a medical exam might require."
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "score": 5,
        "reason": "Deals with 'Reinforcement Learning' for 'Long-Horizon Interactive LLM Agents'. This is relevant for training agents that can handle complex, multi-step tasks and interact effectively, akin to an exam."
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "score": 5,
        "reason": "Focuses on improving 'Reasoning Capability' in LLMs through 'Reinforcement Learning'. Enhanced reasoning is crucial for passing difficult exams."
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "score": 4,
        "reason": "Benchmarking LLMs in 'Web Traversal'. While less direct, web traversal is often a component of information gathering for agents tackling complex knowledge-based tasks."
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "score": 4,
        "reason": "Evaluates 'Retrieval-Augmented Generation' which involves 'Fact Fetching' and 'Reasoning'. These are fundamental skills for answering questions in a comprehensive exam."
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "score": 4,
        "reason": "Focuses on 'Mathematical Reasoning'. While the target is a medical exam, strong reasoning ability is a general prerequisite for complex problem-solving."
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "score": 4,
        "reason": "Introduces a 'benchmark for General AI Assistants'. This aligns with evaluating agents on broad, comprehensive tasks, similar to a medical exam."
      },
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "score": 4,
        "reason": "Introduces the ReAct framework, which combines 'Reasoning and Acting'. This is a foundational concept for intelligent agents that need to perform tasks requiring both understanding and execution."
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
        "score": 3,
        "reason": "Focuses on 'Synthetic Data' and 'Reinforcement Learning' for agents. While potentially useful for training, the direct link to exam benchmarking is weaker than other papers."
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training",
        "score": 3,
        "reason": "Discusses 'Scaling Agents' via 'Continual Pre-training'. This relates to foundational model development, which indirectly supports better agent performance but isn't directly about evaluation or exam tasks."
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI",
        "score": 2,
        "reason": "Argues for the potential of smaller models in agentic AI. This is more of a perspective on model size rather than a direct contribution to benchmarking or solving complex exam tasks."
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "score": 1,
        "reason": "A general technical report for a specific LLM. Without further details on its evaluation or capabilities related to reasoning or exams, its relevance is minimal."
      }
    ]
  },
  "verifier": {
    "rank": 1,
    "ranked": [
      {
        "index": 1,
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
        "score": 9,
        "reason": "Directly relevant to benchmarking LLMs on web browsing tasks, which is a key component for research and information retrieval needed for medical exams."
      },
      {
        "index": 2,
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "score": 8,
        "reason": "Focuses on mathematical reasoning, which can be a significant part of medical exams. Benchmarking these capabilities is relevant."
      },
      {
        "index": 3,
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
        "score": 8,
        "reason": "Addresses retrieval augmentation and long-context LLMs, crucial for handling large datasets and complex information like medical exams."
      },
      {
        "index": 4,
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
        "score": 8,
        "reason": "Focuses on deep research capabilities using web information, directly applicable to gathering information for a comprehensive exam."
      },
      {
        "index": 5,
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "score": 7,
        "reason": "Introduces a framework for agents that combines reasoning and acting, relevant for problem-solving in exams."
      },
      {
        "index": 6,
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "score": 7,
        "reason": "This is a benchmark for general AI assistants, which implies it covers various tasks, potentially including exam-like reasoning."
      },
      {
        "index": 7,
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
        "score": 7,
        "reason": "Focuses on long-horizon search and summarization, useful for processing and understanding extensive medical information."
      },
      {
        "index": 8,
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI",
        "score": 6,
        "reason": "Discusses agentic AI, relevant for building systems that can tackle complex tasks like exams, even if focused on smaller models."
      },
      {
        "index": 9,
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "score": 6,
        "reason": "Focuses on improving reasoning capabilities through reinforcement learning, which is beneficial for exam performance."
      },
      {
        "index": 10,
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "score": 6,
        "reason": "Benchmarks LLMs in web traversal, a skill needed for information gathering for complex exams."
      },
      {
        "index": 11,
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
        "score": 6,
        "reason": "Deals with structuring web-scale evidence for deep research, highly relevant for exam preparation and answering."
      },
      {
        "index": 12,
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "score": 5,
        "reason": "This paper is about an RL system for LLMs, which could be used to train models for better exam performance."
      },
      {
        "index": 13,
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "score": 5,
        "reason": "Evaluates RAG, a common technique for LLMs to access and use external knowledge, crucial for exam tasks."
      },
      {
        "index": 14,
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "score": 5,
        "reason": "A technical report on a specific LLM model, likely including performance evaluations that might relate to reasoning or knowledge tasks."
      },
      {
        "index": 15,
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
        "score": 5,
        "reason": "Focuses on unbounded reasoning in long-horizon agents, relevant for complex, multi-step problem-solving like exams."
      },
      {
        "index": 16,
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "score": 5,
        "reason": "A benchmark for browsing agents, related to information gathering for complex tasks."
      },
      {
        "index": 17,
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "score": 5,
        "reason": "Technical report on foundation models emphasizing agentic, reasoning, and coding capabilities, all relevant to exam performance."
      },
      {
        "index": 18,
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
        "score": 4,
        "reason": "Focuses on web agents and RL, relevant for developing advanced AI capabilities for complex tasks."
      },
      {
        "index": 19,
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "score": 4,
        "reason": "Discusses web agents and superhuman reasoning, potentially applicable to challenging exam questions."
      },
      {
        "index": 20,
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training",
        "score": 4,
        "reason": "Addresses scaling agents, which is relevant for building robust exam-answering systems."
      },
      {
        "index": 21,
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "score": 3,
        "reason": "Focuses on GUI agents and RL, less directly relevant to text-based exams but still within the agentic AI domain."
      },
      {
        "index": 22,
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "score": 3,
        "reason": "RL for long-horizon agents is relevant for complex task solving, but the focus on 'interactive' might be less direct for a static exam."
      },
      {
        "index": 23,
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "score": 7,
        "reason": "Explicitly mentions 'Humanity's Last Exam' and scientific AI agents, making it highly relevant."
      },
      {
        "index": 24,
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling",
        "score": 3,
        "reason": "Discusses general agentic intelligence and environment scaling, which is foundational but less specific to exam performance."
      },
      {
        "index": 25,
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "score": 3,
        "reason": "Focuses on autonomous information seeking, a component of exam preparation, but less directly on reasoning or knowledge application."
      },
      {
        "index": 26,
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "score": 9,
        "reason": "This paper is directly related to 'Humanity's Last Exam', suggesting it's a key benchmark for advanced AI capabilities, likely including medical exams."
      },
      {
        "index": 27,
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
        "score": 2,
        "reason": "Focuses on data synthesizing via information seeking, which is a step removed from direct exam problem-solving."
      }
    ],
    "metrics": {
      "completeness": 1,
      "semanticCorrelation": 0.6662561701895846,
      "correctness": 1
    },
    "raw": {
      "ranked": [
        {
          "index": 1,
          "arxivId": "2504.19314",
          "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
          "score": 9,
          "reason": "Directly relevant to benchmarking LLMs on web browsing tasks, which is a key component for research and information retrieval needed for medical exams."
        },
        {
          "index": 2,
          "arxivId": "2402.03300",
          "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
          "score": 8,
          "reason": "Focuses on mathematical reasoning, which can be a significant part of medical exams. Benchmarking these capabilities is relevant."
        },
        {
          "index": 3,
          "arxivId": "2502.09977",
          "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
          "score": 8,
          "reason": "Addresses retrieval augmentation and long-context LLMs, crucial for handling large datasets and complex information like medical exams."
        },
        {
          "index": 4,
          "arxivId": "2504.21776",
          "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
          "score": 8,
          "reason": "Focuses on deep research capabilities using web information, directly applicable to gathering information for a comprehensive exam."
        },
        {
          "index": 5,
          "arxivId": "2210.03629",
          "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
          "score": 7,
          "reason": "Introduces a framework for agents that combines reasoning and acting, relevant for problem-solving in exams."
        },
        {
          "index": 6,
          "arxivId": "2311.12983",
          "title": "GAIA: a benchmark for General AI Assistants",
          "score": 7,
          "reason": "This is a benchmark for general AI assistants, which implies it covers various tasks, potentially including exam-like reasoning."
        },
        {
          "index": 7,
          "arxivId": "2509.13313",
          "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
          "score": 7,
          "reason": "Focuses on long-horizon search and summarization, useful for processing and understanding extensive medical information."
        },
        {
          "index": 8,
          "arxivId": "2506.02153",
          "title": "Small Language Models are the Future of Agentic AI",
          "score": 6,
          "reason": "Discusses agentic AI, relevant for building systems that can tackle complex tasks like exams, even if focused on smaller models."
        },
        {
          "index": 9,
          "arxivId": "2501.12948",
          "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
          "score": 6,
          "reason": "Focuses on improving reasoning capabilities through reinforcement learning, which is beneficial for exam performance."
        },
        {
          "index": 10,
          "arxivId": "2501.07572",
          "title": "WebWalker: Benchmarking LLMs in Web Traversal",
          "score": 6,
          "reason": "Benchmarks LLMs in web traversal, a skill needed for information gathering for complex exams."
        },
        {
          "index": 11,
          "arxivId": "2509.13312",
          "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
          "score": 6,
          "reason": "Deals with structuring web-scale evidence for deep research, highly relevant for exam preparation and answering."
        },
        {
          "index": 12,
          "arxivId": "2503.14476",
          "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
          "score": 5,
          "reason": "This paper is about an RL system for LLMs, which could be used to train models for better exam performance."
        },
        {
          "index": 13,
          "arxivId": "2409.12941",
          "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
          "score": 5,
          "reason": "Evaluates RAG, a common technique for LLMs to access and use external knowledge, crucial for exam tasks."
        },
        {
          "index": 14,
          "arxivId": "2505.09388",
          "title": "Qwen3 Technical Report",
          "score": 5,
          "reason": "A technical report on a specific LLM model, likely including performance evaluations that might relate to reasoning or knowledge tasks."
        },
        {
          "index": 15,
          "arxivId": "2509.13309",
          "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
          "score": 5,
          "reason": "Focuses on unbounded reasoning in long-horizon agents, relevant for complex, multi-step problem-solving like exams."
        },
        {
          "index": 16,
          "arxivId": "2504.12516",
          "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
          "score": 5,
          "reason": "A benchmark for browsing agents, related to information gathering for complex tasks."
        },
        {
          "index": 17,
          "arxivId": "2508.06471",
          "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
          "score": 5,
          "reason": "Technical report on foundation models emphasizing agentic, reasoning, and coding capabilities, all relevant to exam performance."
        },
        {
          "index": 18,
          "arxivId": "2509.13305",
          "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
          "score": 4,
          "reason": "Focuses on web agents and RL, relevant for developing advanced AI capabilities for complex tasks."
        },
        {
          "index": 19,
          "arxivId": "2507.02592",
          "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
          "score": 4,
          "reason": "Discusses web agents and superhuman reasoning, potentially applicable to challenging exam questions."
        },
        {
          "index": 20,
          "arxivId": "2509.13310",
          "title": "Scaling Agents via Continual Pre-training",
          "score": 4,
          "reason": "Addresses scaling agents, which is relevant for building robust exam-answering systems."
        },
        {
          "index": 21,
          "arxivId": "2509.02544",
          "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
          "score": 3,
          "reason": "Focuses on GUI agents and RL, less directly relevant to text-based exams but still within the agentic AI domain."
        },
        {
          "index": 22,
          "arxivId": "2502.01600",
          "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
          "score": 3,
          "reason": "RL for long-horizon agents is relevant for complex task solving, but the focus on 'interactive' might be less direct for a static exam."
        },
        {
          "index": 23,
          "arxivId": "2507.05241",
          "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
          "score": 7,
          "reason": "Explicitly mentions 'Humanity's Last Exam' and scientific AI agents, making it highly relevant."
        },
        {
          "index": 24,
          "arxivId": "2509.13311",
          "title": "Towards General Agentic Intelligence via Environment Scaling",
          "score": 3,
          "reason": "Discusses general agentic intelligence and environment scaling, which is foundational but less specific to exam performance."
        },
        {
          "index": 25,
          "arxivId": "2505.22648",
          "title": "WebDancer: Towards Autonomous Information Seeking Agency",
          "score": 3,
          "reason": "Focuses on autonomous information seeking, a component of exam preparation, but less directly on reasoning or knowledge application."
        },
        {
          "index": 26,
          "arxivId": "2501.14249",
          "title": "Humanity's Last Exam",
          "score": 9,
          "reason": "This paper is directly related to 'Humanity's Last Exam', suggesting it's a key benchmark for advanced AI capabilities, likely including medical exams."
        },
        {
          "index": 27,
          "arxivId": "2507.15061",
          "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
          "score": 2,
          "reason": "Focuses on data synthesizing via information seeking, which is a step removed from direct exam problem-solving."
        }
      ]
    }
  }
}