{
  "references": {
    "seed": {
      "arxivId": "2510.24701",
      "title": "Tongyi DeepResearch Technical Report"
    },
    "sources": [
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents"
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning"
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research"
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training"
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization"
      },
      {
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling"
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning"
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models"
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization"
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?"
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent"
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI"
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency"
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report"
      },
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability"
      },
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese"
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents"
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing"
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents"
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam"
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal"
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation"
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants"
      },
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      }
    ],
    "selectedSource": {
      "arxivId": "2507.02592",
      "title": "WebSailor: Navigating Super-human Reasoning for Web Agent"
    },
    "target": {
      "arxivId": "1909.08053",
      "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
    }
  },
  "embeddings": {
    "rank": 21,
    "ordered": [
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
        "distance": 0.4152545072319661
      },
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "distance": 0.42515919793159374
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "distance": 0.4352995822221589
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI",
        "distance": 0.4554219501069564
      },
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
        "distance": 0.51748498328686
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "distance": 0.5175810889553498
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "distance": 0.5194806652069011
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
        "distance": 0.522111972955634
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "distance": 0.5529902253465275
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
        "distance": 0.5657464232442008
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training",
        "distance": 0.5722719832626002
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "distance": 0.5733371178273541
      },
      {
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling",
        "distance": 0.6069845897717929
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "distance": 0.6129852823960892
      },
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
        "distance": 0.6173759296331851
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
        "distance": 0.6283340847215821
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "distance": 0.6333825011346181
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
        "distance": 0.6368635620299232
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "distance": 0.6373953436807362
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "distance": 0.6433510945866746
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "distance": 0.6493520002129468
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "distance": 0.6518460179555559
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "distance": 0.6603170006545673
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
        "distance": 0.6991455471761852
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "distance": 0.7023359899631636
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "distance": 0.7215602476428569
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "distance": 0.8181484931825251
      }
    ]
  },
  "llm": {
    "rank": 10,
    "ordered": [
      {
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "score": 10,
        "reason": "This paper introduces the ReAct framework, which is fundamental to agentic behavior by combining reasoning and acting. This is a direct precursor to models like Megatron-LM that aim to scale up reasoning and acting capabilities."
      },
      {
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
        "score": 9,
        "reason": "Focuses on enhancing reasoning capabilities in agents, which aligns with the goal of large language models and the foundation laid by Megatron-LM for handling complex tasks."
      },
      {
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
        "score": 9,
        "reason": "Deals with structuring information for deep research, a capability that requires large-scale models and efficient processing, similar to the goals achieved by Megatron-LM."
      },
      {
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training",
        "score": 8,
        "reason": "Addresses scaling agents through pre-training, a key aspect of building large models like Megatron-LM and enabling them to handle increasingly complex tasks."
      },
      {
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling",
        "score": 8,
        "reason": "Discusses scaling environments for agentic intelligence, which is directly related to the challenges and advancements in training massive models like Megatron-LM."
      },
      {
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "score": 8,
        "reason": "Focuses on advancing agents, particularly with reinforcement learning, which is a critical component in training sophisticated and large-scale language models."
      },
      {
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "score": 8,
        "reason": "This paper discusses advanced foundation models with agentic and reasoning capabilities, directly building upon the infrastructure and scale demonstrated by models like Megatron-LM."
      },
      {
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
        "score": 7,
        "reason": "Explores agentic data synthesis, a process that benefits from and contributes to the development of large-scale models capable of complex information processing."
      },
      {
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "score": 7,
        "reason": "Focuses on building general-purpose AI agents for scientific tasks, which requires the scale and efficiency of models like Megatron-LM."
      },
      {
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "score": 7,
        "reason": "Aims for super-human reasoning in web agents, a goal that aligns with the advancements in large-scale models and their application to complex tasks."
      },
      {
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI",
        "score": 7,
        "reason": "While focused on smaller models, it discusses agentic AI, a field heavily influenced by the scaling principles demonstrated by models like Megatron-LM."
      },
      {
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "score": 7,
        "reason": "Focuses on autonomous information seeking, a task that requires advanced reasoning and large-scale model capabilities."
      },
      {
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
        "score": 7,
        "reason": "Enhancing deep research capabilities in large reasoning models is directly related to the advancements in model parallelism and scale that Megatron-LM represents."
      },
      {
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "score": 6,
        "reason": "Benchmarks for browsing agents are relevant as they push the need for more capable and larger models, which Megatron-LM facilitates."
      },
      {
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
        "score": 6,
        "reason": "Similar to BrowseComp, this benchmark tests LLMs' browsing abilities, indirectly motivating the need for powerful training frameworks like Megatron-LM."
      },
      {
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "score": 6,
        "reason": "Focuses on scaling LLM reinforcement learning, which is a key training methodology for large models."
      },
      {
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
        "score": 6,
        "reason": "Benchmarking RAG and long-context LLMs is relevant to the capabilities that large, parallelized models aim to achieve."
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "score": 6,
        "reason": "This paper directly addresses reinforcement learning for LLM agents, a crucial aspect for developing advanced AI capabilities that require large model training."
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "score": 6,
        "reason": "Focuses on improving reasoning in LLMs via RL, a technique relevant to training large-scale models."
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "score": 5,
        "reason": "Benchmarking web traversal for LLMs is related to the practical applications enabled by large models."
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "score": 5,
        "reason": "Evaluates RAG, a technique often employed by large, capable language models."
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "score": 5,
        "reason": "Focuses on mathematical reasoning, a specific capability that benefits from large model scale and advanced training techniques."
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "score": 5,
        "reason": "A benchmark for general AI assistants touches upon the broad capabilities that large models aim to provide."
      },
      {
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
        "score": 4,
        "reason": "Focuses on agents and scalable RL, relevant to training large models but less directly about the core model parallelism aspect of Megatron-LM."
      },
      {
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
        "score": 4,
        "reason": "Focuses on long-horizon search intelligence and summarization, which are downstream applications of large models."
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "score": 3,
        "reason": "A technical report for a specific LLM, less directly related to the foundational techniques like model parallelism."
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "score": 3,
        "reason": "This paper is a challenging benchmark, but its focus is on evaluation rather than the training methodology or architecture itself."
      },
      {
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "score": 2,
        "reason": "Focuses on RL for agents, which is a training aspect, but less directly on the scaling and parallelism which is key to Megatron-LM."
      },
      {
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "score": 2,
        "reason": "Focuses on RL for reasoning, relevant but not as foundational as model parallelism."
      },
      {
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "score": 2,
        "reason": "Focuses on benchmarking web traversal, which is an application rather than a core training technique for large models."
      },
      {
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "score": 2,
        "reason": "Evaluation of RAG is an application, not a direct precursor to model parallelism."
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "score": 2,
        "reason": "Focuses on mathematical reasoning, a specific task that benefits from large models but doesn't directly relate to the training methodology."
      },
      {
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "score": 2,
        "reason": "A benchmark for AI assistants is an evaluation, not directly related to the scaling techniques."
      },
      {
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "score": 1,
        "reason": "A technical report for a specific LLM, not directly related to the foundational model parallelism concepts."
      },
      {
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "score": 1,
        "reason": "An evaluation benchmark that doesn't focus on the training infrastructure."
      }
    ]
  },
  "verifier": {
    "rank": 22,
    "ranked": [
      {
        "index": 1,
        "arxivId": "2504.19314",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
        "score": 7,
        "reason": "Focuses on web browsing and benchmarking LLMs, which is relevant to agentic capabilities and research tasks, aligning with the target's focus on large model training."
      },
      {
        "index": 2,
        "arxivId": "2210.03629",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "score": 7,
        "reason": "Introduces a framework for combining reasoning and acting, a core concept for advanced language models and agents, which could be a step towards large-scale training methodologies."
      },
      {
        "index": 5,
        "arxivId": "2504.21776",
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
        "score": 6,
        "reason": "Addresses deep research capabilities for large reasoning models, which is conceptually related to the scale and complexity of training models like Megatron-LM."
      },
      {
        "index": 12,
        "arxivId": "2501.07572",
        "title": "WebWalker: Benchmarking LLMs in Web Traversal",
        "score": 6,
        "reason": "Focuses on web traversal for LLMs, a component of agentic behavior and information gathering that could be relevant to large-scale model development and evaluation."
      },
      {
        "index": 17,
        "arxivId": "2409.12941",
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "score": 6,
        "reason": "Evaluates retrieval, fetching, and reasoning, which are foundational skills for advanced language models and could be improved by large-scale training."
      },
      {
        "index": 22,
        "arxivId": "2504.12516",
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "score": 6,
        "reason": "Introduces a benchmark for browsing agents, touching upon agent capabilities and complex tasks that large models are designed to handle."
      },
      {
        "index": 10,
        "arxivId": "2509.13313",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
        "score": 5,
        "reason": "Focuses on long-horizon search and summarization, relevant to complex research tasks that large models aim to solve."
      },
      {
        "index": 11,
        "arxivId": "2509.13310",
        "title": "Scaling Agents via Continual Pre-training",
        "score": 5,
        "reason": "Directly addresses scaling agents through continual pre-training, which is conceptually related to scaling model size and capabilities."
      },
      {
        "index": 13,
        "arxivId": "2509.13311",
        "title": "Towards General Agentic Intelligence via Environment Scaling",
        "score": 5,
        "reason": "Discusses general agentic intelligence and environment scaling, relevant to the challenges and goals of developing very large models."
      },
      {
        "index": 15,
        "arxivId": "2509.13309",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
        "score": 5,
        "reason": "Focuses on unbounded reasoning and long-horizon agents, which are advanced capabilities that large models like Megatron-LM enable."
      },
      {
        "index": 18,
        "arxivId": "2509.13312",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
        "score": 5,
        "reason": "Deals with deep research using web-scale data, a complex task that benefits from powerful, large-scale models."
      },
      {
        "index": 3,
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "score": 4,
        "reason": "Focuses on mathematical reasoning, a specific capability that benefits from advanced model architectures and training, but less directly related to the general training aspect."
      },
      {
        "index": 4,
        "arxivId": "2506.02153",
        "title": "Small Language Models are the Future of Agentic AI",
        "score": 4,
        "reason": "Discusses agentic AI, but the focus on 'small' models makes it less directly relevant to the goal of training multi-billion parameter models."
      },
      {
        "index": 6,
        "arxivId": "2503.14476",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "score": 4,
        "reason": "Focuses on reinforcement learning at scale, which is a training technique that could be applied to large models, but the paper is about RL systems."
      },
      {
        "index": 7,
        "arxivId": "2501.12948",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "score": 4,
        "reason": "Uses reinforcement learning to improve reasoning, a training aspect, but not directly about the challenges of training very large models."
      },
      {
        "index": 8,
        "arxivId": "2502.09977",
        "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
        "score": 4,
        "reason": "Benchmarks RAG and long-context LLMs, focusing on specific capabilities rather than the foundational training of massive models."
      },
      {
        "index": 9,
        "arxivId": "2311.12983",
        "title": "GAIA: a benchmark for General AI Assistants",
        "score": 4,
        "reason": "A benchmark for AI assistants, indicating complex tasks that large models aim to tackle, but not directly about the training methodology."
      },
      {
        "index": 14,
        "arxivId": "2502.01600",
        "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "score": 4,
        "reason": "Applies RL to long-horizon agents, which is a related area but focuses on agent behavior rather than the core model training."
      },
      {
        "index": 16,
        "arxivId": "2509.13305",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
        "score": 4,
        "reason": "Discusses scalable RL and agents, relevant to advanced LLM development, but the focus is on agents and synthetic data."
      },
      {
        "index": 19,
        "arxivId": "2508.06471",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "score": 4,
        "reason": "Covers agentic, reasoning, and coding capabilities of foundation models, which are advanced applications of large models but not directly about training them at scale."
      },
      {
        "index": 20,
        "arxivId": "2509.02544",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
        "score": 4,
        "reason": "Focuses on GUI agents and RL, a specific application area of LLMs, less related to the core training challenges of massive models."
      },
      {
        "index": 21,
        "arxivId": "2507.02592",
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "score": 4,
        "reason": "Focuses on web agents and super-human reasoning, which are advanced outcomes of large models, but not the training process itself."
      },
      {
        "index": 24,
        "arxivId": "2507.15061",
        "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
        "score": 3,
        "reason": "Discusses agentic data synthesis, which is a downstream task or method rather than directly related to training large foundational models."
      },
      {
        "index": 25,
        "arxivId": "2505.09388",
        "title": "Qwen3 Technical Report",
        "score": 3,
        "reason": "A technical report for a specific model, less about the general principles of training massive models like Megatron-LM."
      },
      {
        "index": 26,
        "arxivId": "2505.22648",
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "score": 3,
        "reason": "Focuses on information-seeking agency, which is an application area of LLMs, not directly about their training at scale."
      },
      {
        "index": 23,
        "arxivId": "2507.05241",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "score": 2,
        "reason": "Focuses on scientific AI agents and a specific exam, which is a very specific application and less relevant to general large model training."
      },
      {
        "index": 27,
        "arxivId": "2501.14249",
        "title": "Humanity's Last Exam",
        "score": 1,
        "reason": "This is a benchmark/challenge, not a paper on model training methodology, making it the least relevant for bridging to the target."
      }
    ],
    "metrics": {
      "completeness": 1,
      "semanticCorrelation": 0.5647667521387422,
      "correctness": 1
    },
    "raw": {
      "ranked": [
        {
          "index": 1,
          "arxivId": "2504.19314",
          "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese",
          "score": 7,
          "reason": "Focuses on web browsing and benchmarking LLMs, which is relevant to agentic capabilities and research tasks, aligning with the target's focus on large model training."
        },
        {
          "index": 2,
          "arxivId": "2210.03629",
          "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
          "score": 7,
          "reason": "Introduces a framework for combining reasoning and acting, a core concept for advanced language models and agents, which could be a step towards large-scale training methodologies."
        },
        {
          "index": 5,
          "arxivId": "2504.21776",
          "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
          "score": 6,
          "reason": "Addresses deep research capabilities for large reasoning models, which is conceptually related to the scale and complexity of training models like Megatron-LM."
        },
        {
          "index": 12,
          "arxivId": "2501.07572",
          "title": "WebWalker: Benchmarking LLMs in Web Traversal",
          "score": 6,
          "reason": "Focuses on web traversal for LLMs, a component of agentic behavior and information gathering that could be relevant to large-scale model development and evaluation."
        },
        {
          "index": 17,
          "arxivId": "2409.12941",
          "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
          "score": 6,
          "reason": "Evaluates retrieval, fetching, and reasoning, which are foundational skills for advanced language models and could be improved by large-scale training."
        },
        {
          "index": 22,
          "arxivId": "2504.12516",
          "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
          "score": 6,
          "reason": "Introduces a benchmark for browsing agents, touching upon agent capabilities and complex tasks that large models are designed to handle."
        },
        {
          "index": 10,
          "arxivId": "2509.13313",
          "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
          "score": 5,
          "reason": "Focuses on long-horizon search and summarization, relevant to complex research tasks that large models aim to solve."
        },
        {
          "index": 11,
          "arxivId": "2509.13310",
          "title": "Scaling Agents via Continual Pre-training",
          "score": 5,
          "reason": "Directly addresses scaling agents through continual pre-training, which is conceptually related to scaling model size and capabilities."
        },
        {
          "index": 13,
          "arxivId": "2509.13311",
          "title": "Towards General Agentic Intelligence via Environment Scaling",
          "score": 5,
          "reason": "Discusses general agentic intelligence and environment scaling, relevant to the challenges and goals of developing very large models."
        },
        {
          "index": 15,
          "arxivId": "2509.13309",
          "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents",
          "score": 5,
          "reason": "Focuses on unbounded reasoning and long-horizon agents, which are advanced capabilities that large models like Megatron-LM enable."
        },
        {
          "index": 18,
          "arxivId": "2509.13312",
          "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research",
          "score": 5,
          "reason": "Deals with deep research using web-scale data, a complex task that benefits from powerful, large-scale models."
        },
        {
          "index": 3,
          "arxivId": "2402.03300",
          "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
          "score": 4,
          "reason": "Focuses on mathematical reasoning, a specific capability that benefits from advanced model architectures and training, but less directly related to the general training aspect."
        },
        {
          "index": 4,
          "arxivId": "2506.02153",
          "title": "Small Language Models are the Future of Agentic AI",
          "score": 4,
          "reason": "Discusses agentic AI, but the focus on 'small' models makes it less directly relevant to the goal of training multi-billion parameter models."
        },
        {
          "index": 6,
          "arxivId": "2503.14476",
          "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
          "score": 4,
          "reason": "Focuses on reinforcement learning at scale, which is a training technique that could be applied to large models, but the paper is about RL systems."
        },
        {
          "index": 7,
          "arxivId": "2501.12948",
          "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
          "score": 4,
          "reason": "Uses reinforcement learning to improve reasoning, a training aspect, but not directly about the challenges of training very large models."
        },
        {
          "index": 8,
          "arxivId": "2502.09977",
          "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing",
          "score": 4,
          "reason": "Benchmarks RAG and long-context LLMs, focusing on specific capabilities rather than the foundational training of massive models."
        },
        {
          "index": 9,
          "arxivId": "2311.12983",
          "title": "GAIA: a benchmark for General AI Assistants",
          "score": 4,
          "reason": "A benchmark for AI assistants, indicating complex tasks that large models aim to tackle, but not directly about the training methodology."
        },
        {
          "index": 14,
          "arxivId": "2502.01600",
          "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
          "score": 4,
          "reason": "Applies RL to long-horizon agents, which is a related area but focuses on agent behavior rather than the core model training."
        },
        {
          "index": 16,
          "arxivId": "2509.13305",
          "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning",
          "score": 4,
          "reason": "Discusses scalable RL and agents, relevant to advanced LLM development, but the focus is on agents and synthetic data."
        },
        {
          "index": 19,
          "arxivId": "2508.06471",
          "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
          "score": 4,
          "reason": "Covers agentic, reasoning, and coding capabilities of foundation models, which are advanced applications of large models but not directly about training them at scale."
        },
        {
          "index": 20,
          "arxivId": "2509.02544",
          "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
          "score": 4,
          "reason": "Focuses on GUI agents and RL, a specific application area of LLMs, less related to the core training challenges of massive models."
        },
        {
          "index": 21,
          "arxivId": "2507.02592",
          "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
          "score": 4,
          "reason": "Focuses on web agents and super-human reasoning, which are advanced outcomes of large models, but not the training process itself."
        },
        {
          "index": 24,
          "arxivId": "2507.15061",
          "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
          "score": 3,
          "reason": "Discusses agentic data synthesis, which is a downstream task or method rather than directly related to training large foundational models."
        },
        {
          "index": 25,
          "arxivId": "2505.09388",
          "title": "Qwen3 Technical Report",
          "score": 3,
          "reason": "A technical report for a specific model, less about the general principles of training massive models like Megatron-LM."
        },
        {
          "index": 26,
          "arxivId": "2505.22648",
          "title": "WebDancer: Towards Autonomous Information Seeking Agency",
          "score": 3,
          "reason": "Focuses on information-seeking agency, which is an application area of LLMs, not directly about their training at scale."
        },
        {
          "index": 23,
          "arxivId": "2507.05241",
          "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
          "score": 2,
          "reason": "Focuses on scientific AI agents and a specific exam, which is a very specific application and less relevant to general large model training."
        },
        {
          "index": 27,
          "arxivId": "2501.14249",
          "title": "Humanity's Last Exam",
          "score": 1,
          "reason": "This is a benchmark/challenge, not a paper on model training methodology, making it the least relevant for bridging to the target."
        }
      ]
    }
  }
}