{
  "embeddings": {
    "rank": 4,
    "ordered": [
      {
        "arxivId": "1011.0686",
        "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
        "distance": 0.5251234279191066
      },
      {
        "arxivId": "2504.13958",
        "title": "ToolRL: Reward is All Tool Learning Needs",
        "distance": 0.5307802081880422
      },
      {
        "arxivId": "2403.08140",
        "title": "BAGEL: Bootstrapping Agents by Guiding Exploration with Language",
        "distance": 0.554516885234213
      },
      {
        "arxivId": "2503.09516",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "distance": 0.5592922580739139
      },
      {
        "arxivId": "2303.17651",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "distance": 0.568025408226235
      },
      {
        "arxivId": "2402.11924",
        "title": "CofCA: A STEP-WISE Counterfactual Multi-hop QA benchmark",
        "distance": 0.577972447631188
      },
      {
        "arxivId": "1312.5602",
        "title": "Playing Atari with Deep Reinforcement Learning",
        "distance": 0.5816095644823436
      },
      {
        "arxivId": "1207.4708",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
        "distance": 0.5828918207435345
      },
      {
        "arxivId": "2402.01622",
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "distance": 0.5911609623884833
      },
      {
        "arxivId": "2203.14465",
        "title": "STaR: Bootstrapping Reasoning With Reasoning",
        "distance": 0.5975026153010574
      },
      {
        "arxivId": "2409.19256",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework",
        "distance": 0.5997633671596443
      },
      {
        "arxivId": "1809.01999",
        "title": "Recurrent World Models Facilitate Policy Evolution",
        "distance": 0.6014176399265485
      },
      {
        "arxivId": "2108.00573",
        "title": "♫ MuSiQue: Multihop Questions via Single-hop Question Composition",
        "distance": 0.6015837042355889
      },
      {
        "arxivId": "1809.09600",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
        "distance": 0.6049184242120063
      },
      {
        "arxivId": "2505.10978",
        "title": "Group-in-Group Policy Optimization for LLM Agent Training",
        "distance": 0.6102210463865416
      },
      {
        "arxivId": "2303.11366",
        "title": "Reflexion: language agents with verbal reinforcement learning",
        "distance": 0.6129759739522076
      },
      {
        "arxivId": "2407.18901",
        "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
        "distance": 0.6152942844863907
      },
      {
        "arxivId": "2306.06070",
        "title": "Mind2Web: Towards a Generalist Agent for the Web",
        "distance": 0.6183328476358366
      },
      {
        "arxivId": "1911.08265",
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model",
        "distance": 0.6187558753132785
      },
      {
        "arxivId": "2505.16421",
        "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning",
        "distance": 0.6221979029682577
      },
      {
        "arxivId": "2411.06559",
        "title": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents",
        "distance": 0.6230953439785338
      },
      {
        "arxivId": "2404.07972",
        "title": "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
        "distance": 0.6257832009720009
      },
      {
        "arxivId": "2504.20073",
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning",
        "distance": 0.6280927391968416
      },
      {
        "arxivId": "2305.11854",
        "title": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models",
        "distance": 0.6309572688735319
      },
      {
        "arxivId": "2501.17161",
        "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "distance": 0.6319925919927747
      },
      {
        "arxivId": "1912.01603",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination",
        "distance": 0.634941838738599
      },
      {
        "arxivId": "2406.12045",
        "title": "τ-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains",
        "distance": 0.6352147447641436
      },
      {
        "arxivId": "2502.11357",
        "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents",
        "distance": 0.6362890199959443
      },
      {
        "arxivId": "2411.02337",
        "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning",
        "distance": 0.6370867836717394
      },
      {
        "arxivId": "2410.13825",
        "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents",
        "distance": 0.645135284719588
      },
      {
        "arxivId": "2504.10127",
        "title": "Breaking the Data Barrier - Building GUI Agents Through Task Generalization",
        "distance": 0.6459633723668365
      },
      {
        "arxivId": "2403.13372",
        "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models",
        "distance": 0.6524494626187951
      },
      {
        "arxivId": "2503.01861",
        "title": "Towards Enterprise-Ready Computer Using Generalist Agent",
        "distance": 0.6547230374966835
      },
      {
        "arxivId": "2207.01206",
        "title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
        "distance": 0.6552499170698722
      },
      {
        "arxivId": "2201.11903",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "distance": 0.6575928108009819
      },
      {
        "arxivId": "2408.06327",
        "title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents",
        "distance": 0.6584330485482173
      },
      {
        "arxivId": "2011.01060",
        "title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps",
        "distance": 0.6608428622602883
      },
      {
        "arxivId": "2506.01716",
        "title": "Self-Challenging Language Model Agents",
        "distance": 0.6618528887267459
      },
      {
        "arxivId": "2501.10893",
        "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
        "distance": 0.6635780749512259
      },
      {
        "arxivId": "2210.11610",
        "title": "Large Language Models Can Self-Improve",
        "distance": 0.6640397388377831
      },
      {
        "arxivId": "2412.13194",
        "title": "Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents",
        "distance": 0.6664767302722834
      },
      {
        "arxivId": "2504.01382",
        "title": "An Illusion of Progress? Assessing the Current State of Web Agents",
        "distance": 0.6671127172927922
      },
      {
        "arxivId": "2410.13232",
        "title": "Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation",
        "distance": 0.6674776113071478
      },
      {
        "arxivId": "2010.02193",
        "title": "Mastering Atari with Discrete World Models",
        "distance": 0.6675124415385025
      },
      {
        "arxivId": "2407.01476",
        "title": "Tree Search for Language Model Agents",
        "distance": 0.6699144714695695
      },
      {
        "arxivId": "1806.11532",
        "title": "TextWorld: A Learning Environment for Text-based Games",
        "distance": 0.6704104168562814
      },
      {
        "arxivId": "2307.13854",
        "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
        "distance": 0.6709267553245561
      },
      {
        "arxivId": "2410.12409",
        "title": "Revealing the Barriers of Language Agents in Planning",
        "distance": 0.6732545261434728
      },
      {
        "arxivId": "2506.00320",
        "title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents",
        "distance": 0.6741076340399443
      },
      {
        "arxivId": "2401.01614",
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "distance": 0.6761190283715974
      },
      {
        "arxivId": "2203.07540",
        "title": "ScienceWorld: Is your Agent Smarter than a 5th Grader?",
        "distance": 0.67726031157887
      },
      {
        "arxivId": "2408.03314",
        "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
        "distance": 0.6808016181833019
      },
      {
        "arxivId": "2309.02427",
        "title": "Cognitive Architectures for Language Agents",
        "distance": 0.6825184503408872
      },
      {
        "arxivId": "2504.04736",
        "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
        "distance": 0.6875985128323079
      },
      {
        "arxivId": "2010.03768",
        "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
        "distance": 0.6882677817375781
      },
      {
        "arxivId": "2410.22394",
        "title": "AAAR-1.0: Assessing AI's Potential to Assist Research",
        "distance": 0.6931297242018397
      },
      {
        "arxivId": "2410.05080",
        "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
        "distance": 0.6965280429578711
      },
      {
        "arxivId": "2210.03350",
        "title": "Measuring and Narrowing the Compositionality Gap in Language Models",
        "distance": 0.6969453781506567
      },
      {
        "arxivId": "2312.08914",
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "distance": 0.6973679520935294
      },
      {
        "arxivId": "1707.01495",
        "title": "Hindsight Experience Replay",
        "distance": 0.6996284742142043
      },
      {
        "arxivId": "2506.02918",
        "title": "World Modelling Improves Language Model Agents",
        "distance": 0.7019133898160607
      },
      {
        "arxivId": "2310.01798",
        "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
        "distance": 0.7027295955974844
      },
      {
        "arxivId": "2305.14992",
        "title": "Reasoning with Language Model is Planning with World Model",
        "distance": 0.703554107595744
      },
      {
        "arxivId": "2402.14672",
        "title": "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments",
        "distance": 0.7066332235111754
      },
      {
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "distance": 0.7189548053053879
      },
      {
        "arxivId": "2411.15004",
        "title": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data",
        "distance": 0.7643789218795436
      }
    ]
  },
  "llm": {
    "rank": 14,
    "ordered": [
      {
        "index": 48,
        "arxivId": "2210.11610",
        "title": "Large Language Models Can Self-Improve",
        "score": 8,
        "reason": "Focuses on self-improvement in LLMs, relevant to the target paper's self-optimization approach."
      },
      {
        "index": 47,
        "arxivId": "2303.11366",
        "title": "Reflexion: language agents with verbal reinforcement learning",
        "score": 8,
        "reason": "Introduces a method for LLM agents to use reinforcement learning and self-reflection, aligning with optimization."
      },
      {
        "index": 46,
        "arxivId": "2303.17651",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "score": 7,
        "reason": "Deals with iterative refinement and self-feedback, which is a form of self-optimization."
      },
      {
        "index": 14,
        "arxivId": "2501.17161",
        "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "score": 7,
        "reason": "Compares SFT and RL for post-training, and the target is about RL-based optimization, suggesting a connection in training methodologies."
      },
      {
        "index": 25,
        "arxivId": "2409.19256",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework",
        "score": 7,
        "reason": "RLHF is a form of preference optimization, similar to the target's SimPO."
      },
      {
        "index": 19,
        "arxivId": "2411.02337",
        "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning",
        "score": 6,
        "reason": "Uses RL for training web agents and involves self-evolution, related to optimization."
      },
      {
        "index": 5,
        "arxivId": "2505.10978",
        "title": "Group-in-Group Policy Optimization for LLM Agent Training",
        "score": 6,
        "reason": "Directly mentions 'Policy Optimization' in the context of LLM agents."
      },
      {
        "index": 7,
        "arxivId": "2504.13958",
        "title": "ToolRL: Reward is All Tool Learning Needs",
        "score": 6,
        "reason": "Focuses on RL and rewards, which are central to optimization techniques like SimPO."
      },
      {
        "index": 44,
        "arxivId": "2305.14992",
        "title": "Reasoning with Language Model is Planning with World Model",
        "score": 5,
        "reason": "Connects reasoning, planning, and world models, which can be components of sophisticated optimization agents."
      },
      {
        "index": 51,
        "arxivId": "2203.14465",
        "title": "STaR: Bootstrapping Reasoning With Reasoning",
        "score": 5,
        "reason": "Explores bootstrapping reasoning, a process that can be enhanced by optimization methods."
      },
      {
        "index": 2,
        "arxivId": "2506.01716",
        "title": "Self-Challenging Language Model Agents",
        "score": 5,
        "reason": "Self-challenging can be a strategy for improving performance, akin to optimization."
      },
      {
        "index": 3,
        "arxivId": "2506.00320",
        "title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents",
        "score": 5,
        "reason": "Involves world models and agent actions, relevant to training agents through optimization."
      },
      {
        "index": 9,
        "arxivId": "2504.04736",
        "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
        "score": 5,
        "reason": "Uses multi-step RL for reasoning, which SimPO aims to simplify and optimize."
      },
      {
        "index": 11,
        "arxivId": "2503.09516",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "score": 5,
        "reason": "Employs RL for training LLM agents, a related domain to preference optimization."
      },
      {
        "index": 6,
        "arxivId": "2504.20073",
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning",
        "score": 5,
        "reason": "Discusses self-evolution in LLM agents using RL, which can be seen as a form of optimization."
      },
      {
        "index": 33,
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "score": 4,
        "reason": "Focuses on improving reasoning abilities, which optimization methods can contribute to."
      },
      {
        "index": 58,
        "arxivId": "1912.01603",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination",
        "score": 4,
        "reason": "Involves learning behaviors through imagination, which can be related to optimization goals."
      },
      {
        "index": 59,
        "arxivId": "1911.08265",
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model",
        "score": 4,
        "reason": "Uses planning with learned models for mastering tasks, relevant to agent optimization."
      },
      {
        "index": 1,
        "arxivId": "2506.02918",
        "title": "World Modelling Improves Language Model Agents",
        "score": 4,
        "reason": "Focuses on world modeling for agents, a foundation for many advanced training techniques including optimization."
      },
      {
        "index": 22,
        "arxivId": "2410.13232",
        "title": "Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation",
        "score": 4,
        "reason": "Similar to [2506.02918], emphasizes world models for web agents."
      },
      {
        "index": 57,
        "arxivId": "2010.02193",
        "title": "Mastering Atari with Discrete World Models",
        "score": 4,
        "reason": "Applies world models to reinforcement learning for mastering tasks."
      },
      {
        "index": 61,
        "arxivId": "1809.01999",
        "title": "Recurrent World Models Facilitate Policy Evolution",
        "score": 4,
        "reason": "Connects world models to policy evolution, which is a type of optimization."
      },
      {
        "index": 43,
        "arxivId": "2306.06070",
        "title": "Mind2Web: Towards a Generalist Agent for the Web",
        "score": 4,
        "reason": "Aims for a generalist agent for web tasks, where optimization techniques are crucial for performance."
      },
      {
        "index": 42,
        "arxivId": "2307.13854",
        "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
        "score": 4,
        "reason": "Focuses on building autonomous agents for web environments, a common application area for RL optimization."
      },
      {
        "index": 18,
        "arxivId": "2411.06559",
        "title": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents",
        "score": 4,
        "reason": "Explores world models for web agents and model-based planning."
      },
      {
        "index": 29,
        "arxivId": "2407.01476",
        "title": "Tree Search for Language Model Agents",
        "score": 4,
        "reason": "Tree search is a planning method often used in conjunction with optimization or to improve agent performance."
      },
      {
        "index": 4,
        "arxivId": "2505.16421",
        "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning",
        "score": 4,
        "reason": "Uses RL for training web agents, a common use case for optimization methods."
      },
      {
        "index": 13,
        "arxivId": "2502.11357",
        "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents",
        "score": 4,
        "reason": "Focuses on web agents and exploration, which is related to RL and optimization."
      },
      {
        "index": 8,
        "arxivId": "2504.10127",
        "title": "Breaking the Data Barrier - Building GUI Agents Through Task Generalization",
        "score": 4,
        "reason": "Focuses on building agents and task generalization, areas where optimization is applied."
      },
      {
        "index": 23,
        "arxivId": "2410.12409",
        "title": "Revealing the Barriers of Language Agents in Planning",
        "score": 4,
        "reason": "Addresses planning for language agents, which optimization can improve."
      },
      {
        "index": 34,
        "arxivId": "2402.14672",
        "title": "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments",
        "score": 4,
        "reason": "Discusses tools for language agents, which can be optimized for better performance."
      },
      {
        "index": 30,
        "arxivId": "2406.12045",
        "title": "τ-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains",
        "score": 4,
        "reason": "Benchmarks tool-agent interaction, suggesting a need for efficient and optimized agents."
      },
      {
        "index": 56,
        "arxivId": "2010.03768",
        "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
        "score": 4,
        "reason": "Focuses on interactive learning in environments, where optimization techniques are often used."
      },
      {
        "index": 52,
        "arxivId": "2203.07540",
        "title": "ScienceWorld: Is your Agent Smarter than a 5th Grader?",
        "score": 4,
        "reason": "Evaluates agent intelligence in an environment, implying performance optimization."
      },
      {
        "index": 62,
        "arxivId": "1806.11532",
        "title": "TextWorld: A Learning Environment for Text-based Games",
        "score": 4,
        "reason": "A text-based environment for learning agents, where optimization is key."
      },
      {
        "index": 64,
        "arxivId": "1312.5602",
        "title": "Playing Atari with Deep Reinforcement Learning",
        "score": 4,
        "reason": "A foundational paper in deep RL, which SimPO is a part of."
      },
      {
        "index": 65,
        "arxivId": "1207.4708",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
        "score": 4,
        "reason": "An evaluation platform for general agents, where optimization is crucial for state-of-the-art performance."
      },
      {
        "index": 10,
        "arxivId": "2504.01382",
        "title": "An Illusion of Progress? Assessing the Current State of Web Agents",
        "score": 3,
        "reason": "Critically assesses web agents, suggesting a need for improved methods like SimPO."
      },
      {
        "index": 12,
        "arxivId": "2503.01861",
        "title": "Towards Enterprise-Ready Computer Using Generalist Agent",
        "score": 3,
        "reason": "Aims for enterprise-ready agents, implying a need for robust and efficient methods."
      },
      {
        "index": 15,
        "arxivId": "2501.10893",
        "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
        "score": 3,
        "reason": "Focuses on self-adaptive agents, which can be enhanced by optimization."
      },
      {
        "index": 16,
        "arxivId": "2412.13194",
        "title": "Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents",
        "score": 3,
        "reason": "Involves a multi-component agent system, where optimization plays a role."
      },
      {
        "index": 17,
        "arxivId": "2411.15004",
        "title": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data",
        "score": 3,
        "reason": "Focuses on specialized web agents, where performance tuning (optimization) is important."
      },
      {
        "index": 21,
        "arxivId": "2410.13825",
        "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents",
        "score": 3,
        "reason": "Proposes a baseline for LLM web agents, indicating a need for strong performance which optimization can provide."
      },
      {
        "index": 26,
        "arxivId": "2408.06327",
        "title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents",
        "score": 3,
        "reason": "Benchmarks multimodal agents; optimized agents perform better."
      },
      {
        "index": 27,
        "arxivId": "2408.03314",
        "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
        "score": 3,
        "reason": "Discusses optimal scaling and efficiency, concepts related to optimization."
      },
      {
        "index": 28,
        "arxivId": "2407.18901",
        "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
        "score": 3,
        "reason": "Benchmarks interactive agents, where optimization is a key driver of performance."
      },
      {
        "index": 31,
        "arxivId": "2404.07972",
        "title": "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
        "score": 3,
        "reason": "Benchmarks agents in real environments, suggesting the need for optimized agents."
      },
      {
        "index": 37,
        "arxivId": "2402.01622",
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "score": 3,
        "reason": "Benchmarks planning for language agents, where SimPO could be applied."
      },
      {
        "index": 38,
        "arxivId": "2401.01614",
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "score": 3,
        "reason": "Discusses generalist web agents, implying the need for optimized performance."
      },
      {
        "index": 39,
        "arxivId": "2312.08914",
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "score": 3,
        "reason": "Focuses on GUI agents, a domain where optimization can improve task success."
      },
      {
        "index": 41,
        "arxivId": "2309.02427",
        "title": "Cognitive Architectures for Language Agents",
        "score": 3,
        "reason": "Discusses agent architectures, which can be optimized for better function."
      },
      {
        "index": 45,
        "arxivId": "2305.11854",
        "title": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models",
        "score": 3,
        "reason": "Focuses on web navigation and instruction finetuning, related to agent training and performance."
      },
      {
        "index": 50,
        "arxivId": "2207.01206",
        "title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
        "score": 3,
        "reason": "Deals with web interaction for language agents, a field where optimization is applied."
      },
      {
        "index": 53,
        "arxivId": "2201.11903",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "score": 3,
        "reason": "Focuses on eliciting reasoning, which can be improved by optimization methods."
      },
      {
        "index": 20,
        "arxivId": "2410.22394",
        "title": "AAAR-1.0: Assessing AI's Potential to Assist Research",
        "score": 2,
        "reason": "Broadly assesses AI's potential, less directly related to optimization techniques."
      },
      {
        "index": 24,
        "arxivId": "2410.05080",
        "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
        "score": 2,
        "reason": "Benchmarks scientific discovery agents, implying a need for improved performance."
      },
      {
        "index": 28,
        "arxivId": "2407.18901",
        "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
        "score": 2,
        "reason": "Benchmarks interactive agents, where optimization is a key driver of performance."
      },
      {
        "index": 32,
        "arxivId": "2403.13372",
        "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models",
        "score": 2,
        "reason": "Focuses on efficient fine-tuning, a different aspect of LLM training than optimization."
      },
      {
        "index": 35,
        "arxivId": "2402.11924",
        "title": "CofCA: A STEP-WISE Counterfactual Multi-hop QA benchmark",
        "score": 2,
        "reason": "Focuses on QA benchmarks, less directly related to RL optimization."
      },
      {
        "index": 36,
        "arxivId": "2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "score": 2,
        "reason": "Focuses on improving reasoning abilities, which optimization methods can contribute to."
      },
      {
        "index": 40,
        "arxivId": "2310.01798",
        "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
        "score": 2,
        "reason": "Discusses limitations of LLMs, but not directly about optimization techniques."
      },
      {
        "index": 49,
        "arxivId": "2210.03350",
        "title": "Measuring and Narrowing the Compositionality Gap in Language Models",
        "score": 2,
        "reason": "Focuses on compositionality, a different aspect of LLM evaluation."
      },
      {
        "index": 54,
        "arxivId": "2108.00573",
        "title": "♫ MuSiQue: Multihop Questions via Single-hop Question Composition",
        "score": 2,
        "reason": "Focuses on a QA dataset, not directly on optimization methods."
      },
      {
        "index": 55,
        "arxivId": "2011.01060",
        "title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps",
        "score": 2,
        "reason": "Focuses on QA dataset construction."
      },
      {
        "index": 60,
        "arxivId": "1809.09600",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
        "score": 2,
        "reason": "A QA dataset, not directly related to SimPO."
      },
      {
        "index": 1,
        "arxivId": "2506.02918",
        "title": "World Modelling Improves Language Model Agents",
        "score": 1,
        "reason": "General topic of world models for agents, but not specific to preference optimization."
      },
      {
        "index": 63,
        "arxivId": "1707.01495",
        "title": "Hindsight Experience Replay",
        "score": 1,
        "reason": "A specific RL technique (HER), but not directly related to preference optimization."
      },
      {
        "index": 4,
        "arxivId": "2505.16421",
        "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning",
        "score": 1,
        "reason": "Reinforcement learning for web agents, but not specific to preference optimization."
      },
      {
        "index": 6,
        "arxivId": "2504.20073",
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning",
        "score": 1,
        "reason": "Self-evolution in RL agents, but not specific to preference optimization."
      },
      {
        "index": 11,
        "arxivId": "2503.09516",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "score": 1,
        "reason": "RL for LLM agents, but not specifically preference optimization."
      },
      {
        "index": 15,
        "arxivId": "2501.10893",
        "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
        "score": 1,
        "reason": "Self-adaptive agents, not directly preference optimization."
      },
      {
        "index": 18,
        "arxivId": "2411.06559",
        "title": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents",
        "score": 1,
        "reason": "World models and planning, but not preference optimization."
      },
      {
        "index": 19,
        "arxivId": "2411.02337",
        "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning",
        "score": 1,
        "reason": "RL for web agents, not specifically preference optimization."
      },
      {
        "index": 56,
        "arxivId": "2010.03768",
        "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
        "score": 1,
        "reason": "Interactive learning environments, not specific to preference optimization."
      },
      {
        "index": 57,
        "arxivId": "2010.02193",
        "title": "Mastering Atari with Discrete World Models",
        "score": 1,
        "reason": "World models for RL, but not preference optimization."
      },
      {
        "index": 61,
        "arxivId": "1809.01999",
        "title": "Recurrent World Models Facilitate Policy Evolution",
        "score": 1,
        "reason": "World models and policy evolution, not preference optimization."
      },
      {
        "index": 64,
        "arxivId": "1312.5602",
        "title": "Playing Atari with Deep Reinforcement Learning",
        "score": 1,
        "reason": "Foundational RL paper, but not preference optimization."
      }
    ]
  }
}